{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cf480b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"c:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m cloudpickle\u001b[39m.\u001b[39;49mdumps(obj, pickle_protocol)\n\u001b[0;32m    459\u001b[0m \u001b[39mexcept\u001b[39;00m pickle\u001b[39m.\u001b[39mPickleError:\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     70\u001b[0m cp \u001b[39m=\u001b[39m CloudPickler(\n\u001b[0;32m     71\u001b[0m     file, protocol\u001b[39m=\u001b[39mprotocol, buffer_callback\u001b[39m=\u001b[39mbuffer_callback\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m cp\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[0;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 602\u001b[0m     \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[0;32m    603\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:692\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, types\u001b[39m.\u001b[39mFunctionType):\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_reduce(obj)\n\u001b[0;32m    693\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     \u001b[39m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[0;32m    695\u001b[0m     \u001b[39m# dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:565\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dynamic_function_reduce(obj)\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:546\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    545\u001b[0m newargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_getnewargs(func)\n\u001b[1;32m--> 546\u001b[0m state \u001b[39m=\u001b[39m _function_getstate(func)\n\u001b[0;32m    547\u001b[0m \u001b[39mreturn\u001b[39;00m (types\u001b[39m.\u001b[39mFunctionType, newargs, state, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    548\u001b[0m         _function_setstate)\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    146\u001b[0m slotstate \u001b[39m=\u001b[39m {\n\u001b[0;32m    147\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m\"\u001b[39m: func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[0;32m    148\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m\"\u001b[39m: func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m__closure__\u001b[39m\u001b[39m\"\u001b[39m: func\u001b[39m.\u001b[39m\u001b[39m__closure__\u001b[39m,\n\u001b[0;32m    155\u001b[0m }\n\u001b[1;32m--> 157\u001b[0m f_globals_ref \u001b[39m=\u001b[39m _extract_code_globals(func\u001b[39m.\u001b[39;49m\u001b[39m__code__\u001b[39;49m)\n\u001b[0;32m    158\u001b[0m f_globals \u001b[39m=\u001b[39m {k: func\u001b[39m.\u001b[39m\u001b[39m__globals__\u001b[39m[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m f_globals_ref \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m\n\u001b[0;32m    159\u001b[0m              func\u001b[39m.\u001b[39m\u001b[39m__globals__\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:334\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[1;34m(co)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m out_names \u001b[39m=\u001b[39m {names[oparg]: \u001b[39mNone\u001b[39;49;00m \u001b[39mfor\u001b[39;49;00m _, oparg \u001b[39min\u001b[39;49;00m _walk_global_ops(co)}\n\u001b[0;32m    336\u001b[0m \u001b[39m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[39m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[39m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[39m# add the result to code_globals\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:334\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m out_names \u001b[39m=\u001b[39m {names[oparg]: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m _, oparg \u001b[39min\u001b[39;00m _walk_global_ops(co)}\n\u001b[0;32m    336\u001b[0m \u001b[39m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[39m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[39m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[39m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m myRDD\u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mparallelize(data)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Visualitza-ho\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m myRDD\u001b[39m.\u001b[39;49mtake(\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1882\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 1883\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[0;32m   1885\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[0;32m   1886\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   1485\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD\u001b[39m.\u001b[39mrunJob(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[0;32m   1487\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3502\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3503\u001b[0m     profiler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 3505\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(\n\u001b[0;32m   3506\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prev_jrdd_deserializer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd_deserializer, profiler\n\u001b[0;32m   3507\u001b[0m )\n\u001b[0;32m   3509\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3510\u001b[0m python_rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD(\n\u001b[0;32m   3511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev_jrdd\u001b[39m.\u001b[39mrdd(), wrapped_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreservesPartitioning, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_barrier\n\u001b[0;32m   3512\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[39massert\u001b[39;00m serializer, \u001b[39m\"\u001b[39m\u001b[39mserializer should not be empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3361\u001b[0m command \u001b[39m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[1;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m   3363\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3364\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonFunction(\n\u001b[0;32m   3365\u001b[0m     \u001b[39mbytearray\u001b[39m(pickled_command),\n\u001b[0;32m   3366\u001b[0m     env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3371\u001b[0m     sc\u001b[39m.\u001b[39m_javaAccumulator,\n\u001b[0;32m   3372\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command)\u001b[0m\n\u001b[0;32m   3342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_for_python_RDD\u001b[39m(sc: \u001b[39m\"\u001b[39m\u001b[39mSparkContext\u001b[39m\u001b[39m\"\u001b[39m, command: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mbytes\u001b[39m, Any, Any, Any]:\n\u001b[0;32m   3343\u001b[0m     \u001b[39m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[0;32m   3344\u001b[0m     ser \u001b[39m=\u001b[39m CloudPickleSerializer()\n\u001b[1;32m-> 3345\u001b[0m     pickled_command \u001b[39m=\u001b[39m ser\u001b[39m.\u001b[39;49mdumps(command)\n\u001b[0;32m   3346\u001b[0m     \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3347\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pickled_command) \u001b[39m>\u001b[39m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mgetBroadcastThreshold(sc\u001b[39m.\u001b[39m_jsc):  \u001b[39m# Default 1M\u001b[39;00m\n\u001b[0;32m   3348\u001b[0m         \u001b[39m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marc Martinez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    466\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCould not serialize object: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (e\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, emsg)\n\u001b[0;32m    467\u001b[0m print_exec(sys\u001b[39m.\u001b[39mstderr)\n\u001b[1;32m--> 468\u001b[0m \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mPicklingError(msg)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Import findpark desde java para ejecutar, porke el sistema no lo encuentra.\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "#Create SparkSession\n",
    "sc= SparkContext(appName = 'appname')\n",
    "# Completa-ho, per exemple a ‘ASIX’ hi ha 20, ‘SMX’ hi ha19, ‘DAM’ hi ha 17 i ‘DAW’ hi ha 21, pista (Python List of arrays)\n",
    "data = [(\"SMX1A\", \"20\"), (\"SMX1B\", \"19\"), (\"SMX1C\", \"21\"),(\"DAM\",\"17\")]\n",
    "myRDD= sc.parallelize(data)\n",
    "\n",
    "# Visualitza-ho\n",
    "myRDD.take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf71fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\"sepal.length\"',\n",
       "  '\"sepal.width\"',\n",
       "  '\"petal.length\"',\n",
       "  '\"petal.width\"',\n",
       "  '\"variety\"'],\n",
       " ['5.1', '3.5', '1.4', '.2', '\"Setosa\"'],\n",
       " ['4.9', '3', '1.4', '.2', '\"Setosa\"'],\n",
       " ['4.7', '3.2', '1.3', '.2', '\"Setosa\"'],\n",
       " ['4.6', '3.1', '1.5', '.2', '\"Setosa\"']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Completa els 2 espais proposats\n",
    "myRDD= sc.textFile(\"iris.csv\").map(lambda element: element.split(\",\"))\n",
    "# Visualitza-ho\n",
    "myRDD.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758285a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bdcb8-5d37-43ed-9790-262494c73a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd2993-29c3-4b84-8f25-b1f010d5fb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[13] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "myRDD= sc.textFile(\"2008_all_states.csv\",minPartitions =5, use_unicode=True).map(lambda element: element.split(\",\")).sample(0.25,123)).take(10)\n",
    "print(myRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300a424-f456-453a-8697-d47fee52b871",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'getNumPartitions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mD:\\Temp\\ipykernel_6532\\1729622735.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'getNumPartitions'"
     ]
    }
   ],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b42f531-aec4-4166-8063-977fae3718f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
