{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZonF169hNzbd",
        "outputId": "7e5a920c-372d-4605-bc68-652f0e04b744"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=9b7de290824aec3a1d143d323faefd04294329b7796e73e8d942865e4f4fc8ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprovar si hi ha una sessió creada"
      ],
      "metadata": {
        "id": "-6QGzCLoNhwN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g9Ji1K3zNdCf"
      },
      "outputs": [],
      "source": [
        "# Import SparkSession from pyspark.sql\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalment utilitzarem la paraula spark pero tambe la podem \"customitzar\".\n",
        "\n",
        "L'ordre és get o Create perquè si n'hi ha una sessió no es crea una nova"
      ],
      "metadata": {
        "id": "Xw7QlbCUPU8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear my_spark\n",
        "my_spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "V5IpGXfKO3vP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fem un Print"
      ],
      "metadata": {
        "id": "xUG3YgecPv7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print de la sessió\n",
        "print(my_spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kbONazXP2_f",
        "outputId": "a6b20b7e-6a4b-4631-a297-d46462d3a50d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f313e980e20>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afegim més llibreries, us sona?"
      ],
      "metadata": {
        "id": "-9eAzvxMR8PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "rllNRavoSFIL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crearem un Pandas DataFrame, mai confondre amb un Spark Datafreme. Us encoratjo a investigar la diferència."
      ],
      "metadata": {
        "id": "D0ZviMyMTjO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_temp = pd.DataFrame(np.random.random(10))"
      ],
      "metadata": {
        "id": "pRkiBQ8aT79E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crearem ara un Spark DataFrame"
      ],
      "metadata": {
        "id": "W17y_hByUgny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conseguim spark_temp a partir de pd_temp\n",
        "spark_temp = my_spark.createDataFrame(pd_temp)"
      ],
      "metadata": {
        "id": "OdjeIuL8Ura7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenim un DataFrame creat amb aquesta sessió que hem anomenat my_spark, però com tenim el Catalog?"
      ],
      "metadata": {
        "id": "dADv5wSrVT15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZwWgBYvVhI8",
        "outputId": "24f4c8a3-5168-4039-c516-b31c4da7ccfa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ens surt buit perquè no hem afegit res al Catalog.\n",
        "\n",
        "Ara anem a afegir \"temp\" al Catalog"
      ],
      "metadata": {
        "id": "K2gQl24AWFlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_temp.createOrReplaceTempView(\"temp\")"
      ],
      "metadata": {
        "id": "4OwjYQSSWOmc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tornem a comprovar el Catalog"
      ],
      "metadata": {
        "id": "f59uaBM6Wmp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP7nN3PqW78S",
        "outputId": "f26ec2d6-9dbb-4ea5-fa89-b83011949cd9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ara ja tenim una \"Vista\" accessible a tots els scripts que corren en els nodes del cluster"
      ],
      "metadata": {
        "id": "Jgf2RSXhXYbl"
      }
    }
  ]
}